@InProceedings{cornish2020relaxing,
  title = 	 {Relaxing Bijectivity Constraints with Continuously Indexed Normalising Flows},
  author =       {Cornish, Rob and Caterini, Anthony and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2133--2143},
  year = 	 {2020},
  editor = 	 {III, Hal Daum√© and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/cornish20a/cornish20a.pdf},
  url = 	 {http://proceedings.mlr.press/v119/cornish20a.html},
  abstract = 	 {We show that normalising flows become pathological when used to model targets whose supports have complicated topologies. In this scenario, we prove that a flow must become arbitrarily numerically noninvertible in order to approximate the target closely. This result has implications for all flow-based models, and especially residual flows (ResFlows), which explicitly control the Lipschitz constant of the bijection used. To address this, we propose continuously indexed flows (CIFs), which replace the single bijection used by normalising flows with a continuously indexed family of bijections, and which can intuitively "clean up" mass that would otherwise be misplaced by a single bijection. We show theoretically that CIFs are not subject to the same topological limitations as normalising flows, and obtain better empirical performance on a variety of models and benchmarks.}
}
